---
title: "Visualizing the Fisher Information and the Variance of the MLE"
output:
  pdf_document: 
      number_sections: true
bibliography: references.bib
author: "Lukas Stein"
---

```{r, include=FALSE}
library(miscTools)
library(maxLik)
library(scales)
library(gridExtra)
library(grid)
library(knitr)
library(latex2exp)
library(shape)
source("csa_functions.R")
```

# Introduction

The aim of this work is to provide a graphical interpretation of the Variance of the MLE. Since the Variance of the MLE is inversely related to the Fisher Information (FI) of the respective distribution, I will mainly focus on deriving a visual intuition for the FI. In chapter 2, I will present two methods of calculating the FI and briefly explain their relevance to the following chapters. In 3.1 I will give some general information about the Cauchy Distribution and it's properties. The main section of this work deals with the visualization of the Cauchy distribution's log-likelihood and its derivatives in 3.2. In 4 I will discuss the implications of picking a false minimum to evaluate the log-likelihood at.

# The Variance and the Fisher Information
The theorem of the asymptotic variance of the MLE states that for any extremum estimator $\hat{\theta}_N$ for the true parameter $\theta$, the asymptotic distribution is given by:
$$
\hat{\theta}\overset{d}{\rightarrow}N(\theta ,\frac{I(\theta)^{-1}}{N})
$$
where $I(\theta)$ is the Fisher Information, which is defined as the variance of the score function^[We define the score function as the first derivative of the likelihood function with respect to $\theta$]:
$$
\tag{1}
I(\theta)=Var(\frac{d}{d\theta}\mathrm{log}L(x|\theta))=\int(\frac{d}{d\theta}\mathrm{log}f(x|\theta))^2p_\theta(x)dx,\;\;\;\;\mathrm{with}\;p_\theta(x)= f(x|\theta)
$$
An interpretation of this is that the Fisher Information as a function of $\theta$ describes the sensitivity of the log-likelihood function $\mathrm{log}L$ to changes in $\theta$ (given by the squared derivative of the log likelihood with respect to $\theta$), for all possible values of $x\in X$, weighted by their respective probability under $\theta$. It therefore describes the _expected_ sensitivity of the log likelihood function to changes in $\theta$. 

Under mild regularity conditions it can be shown that this is equivalent to [@ly2017tutorial]:
$$
\tag{2}
I(\theta)=-E_\theta(\frac{d^2}{d\theta^2}\mathrm{log}f(x|\theta))=-\int(\frac{d}{d\theta}\mathrm{log}f(x|\theta))^2p_\theta(x)dx
$$
Since we do not know the true parameter value $\theta$, we can use the aforementioned formula to _approximate_ the Fisher Information using our empirical data^[In the multidimensional case, the observed Fisher Information is equal to the Hessian of the negative log likelihood.]:
$$
\tag{3}
J_n(\theta)
=\frac{d^2}{d\theta^2}\mathrm{log}L(x|\hat{\theta})
=\sum_{i=1}^{N}\frac{d^2}{d\theta^2}\mathrm{log}f(x_i|\hat{\theta})
\\\overset{p}{\rightarrow}\;\;N\cdot\int(\frac{d}{d\theta}\mathrm{log}f(x|\theta))^2p_\theta(x)dx=N\cdot I(\theta)
$$

In the following chapters, I will explore how these expressions can be interpreted graphically.

\newpage
# Cauchy Distribution Likelihood
## General Information about the Cauchy Distribution
The Cauchy Distribution not only makes for a relatively simple example of a (potentially) 'bumpy' likelihood function, but also has the added advantage of being relevant in real-life modeling, e.g., of stock returns on financial markets. It is often used in problems that generalize to most multimodal likelihood functions. Similarly to the normal distribution, the entire family of Cauchy distributions only differs in its two parameters: The location $\theta$ (also: $x_0$, analogous to $\mu$) and size $\gamma$ (analogous to $\sigma^2$)
^[To express that a random variable X follows a Cauchy distribution with location $\theta$ and size $\gamma$, we write:
$$X \sim Ca(\theta, \gamma)$$
]:
$$
f(x,\gamma,\theta)=\frac{1}{\gamma\pi}\frac{1}{1+(\frac{x-\theta}{\gamma})^2}
$$
However, compared to the normal distribution, the Cauchy Distribution has relatively fat tails. This makes estimators like the mean, and the variance essentially useless when it comes to estimating the true parameters of the distribution from a sample. The log likelihood is:
$$
\mathrm{log}L(x^n,\theta,\gamma)=n\mathrm{log}(\gamma)-n\mathrm{log}(\pi)-\sum_{n}^{i=1}\mathrm{log}[\gamma^2+(x_i-\theta)^2]
$$
Forming the derivative with respect to $\theta$ yields:
$$
\frac{\delta\mathrm{log}L}{\delta\theta}=\sum_{i=1}^{n}\frac{2(x_i-\theta)}{\gamma^2+(x_i-\theta)^2}=0
$$
The equation above suggests the possibility of multiple local maxima. As Barnett (1966) explains, “[…] the fact that multiple relative maxima may occur is obvious from the form of $\delta l/\delta\theta$ [...]; $\delta l/\delta\theta$ tends to zero, from below when $\theta\rightarrow+\infty$ and from above when $\theta\rightarrow-\infty$, whilst any 'relatively isolated' observation $x_i$ must have the effect of making $\delta l/\delta\theta$ pass through zero from above, implying the presence of a relative maximum.” [@10.1093/biomet/53.1-2.151]

The second derivative w.r.t. $\theta$:
$$
\frac{\partial^2 \textrm{log}L}{\partial \theta^2}=\sum_{i=1}^{n}\frac{2(\theta-\gamma-x_i)}{[(\theta-x_i)^2+\gamma^2]^2}
$$


## Visualizing the Fisher Information on three different levels

To compare how the size parameter $\gamma$ and the sample size $n$ affect the Fisher Information contained in a sample, we will generate three different "sample types" that only differ in these two variables:

* Type 1: $x^{n=100} \overset{iid}{\sim} Ca(0,100)$
* Type 2: $x^{n=  7} \overset{iid}{\sim} Ca(0,100)$
* Type 3: $x^{n=100} \overset{iid}{\sim} Ca(0, 50)$

Of each type, I will generate 200 samples. To be able to discuss specific aspects of the likelihood function, without relying too heavily on a single example, I will (only) graph the log-likelihoods of the first 15 samples generated for each type. For the remaining samples, I will only plot the density of values at the true parameter value. Assuming that this empirical density is an asymptotic approximation of the analytical density, also allows me to make visual arguments about the definition of the FI given by Equation (1).

```{r, include=FALSE}
seed_seed = 0 # for more control over the generated samples
iterations = 200 # how many samples will be generated (of each type)
plotted_iterations = 15 # how many samples will be plotted (of each type)
```

The following analysis will follow the ideas and methods outlined in @zhengx and @yt_mi21. We will derive a visual intuition for the Fisher Information on three different levels by looking at the (1) log-likelihood, (2) it's first derivative w.r.t. $\theta$, the score function, and (3) it's second derivative.

```{r, echo=FALSE}
loc = 0     # location parameter stays the same across all samples

n_1 = 100   # n for Type 1 and 3
n_2 = 10    # n for Type 2
gam_1 = 1   # gamma for Type 1 and 2
gam_2 = 5   # gamma for Type 3

loc_length = 1001 # determines at how many points we evaluate the functions!
loc_range <- seq(loc-50,loc+50,length.out=loc_length) # create x-value grid

# generate Type 1 Samples
samples_cauy_1 <- c()
for (i in 1:iterations){
  set.seed(seed_seed + i)
  sample = rcauchy(n_1, loc, gam_1)
  samples_cauy_1 = cbind(samples_cauy_1, sample)
  colnames(samples_cauy_1)[i] = paste("x", as.character(i), sep="")
}

# generate Type 2 Samples
samples_cauy_2 <- c()
for (i in 1:iterations){
  set.seed(seed_seed + i)
  sample = rcauchy(n_2, loc, gam_1)
  samples_cauy_2 = cbind(samples_cauy_2, sample)
  colnames(samples_cauy_2)[i] = paste("x", as.character(i), sep="")
}

# generate Type 3 Samples
samples_cauy_3 <- c()
for (i in 1:iterations){
  set.seed(seed_seed + i)
  sample = rcauchy(n_1, loc, gam_2)
  samples_cauy_3 = cbind(samples_cauy_3, sample)
  colnames(samples_cauy_3)[i] = paste("x", as.character(i), sep="")
}
```


### Level 1: The likelihood function
We are now plotting the log-likelihood functions for the first couple (~15) of samples (left, adjusted by their global minimum). Additionally, we can use a kernel estimator to estimate the distribution of values of the likelihood function, evaluated at the true value of $\theta$ for the different samples (right). 

Each row corresponds to one of the three sample types defined above. 

```{r, echo=FALSE, fig.cap="log-likelihood functions for the first 15 samples (left) and kernel density estimation over the likelihood functions of all samples evaluated at the true parameter value (right). Likelihood functions adjusted so their global minima lie on the x-axis."}
par(mfrow=c(3,2), mar=c(4.1, 4.1, 2.1, 2.1))

# I use separate methods to plot the log-likelihood and its first and second derivative. A method always creates two different plots: The log-likelihood (or its derivative) for the first 15 samples and the density of values at the true theta for all samples.
plot_ll_cauy <- function(samples_cauy, loc_v, gam_v, ylim){
  ll_values_cauy = c()
  loc_position = which.min(abs(loc_range-loc_v)) # index location of true theta
  
  for (i in 1:ncol(samples_cauy)){  
    
    # calculate likelihood function values for each sample and store them in 
    # ll_values_cauy
    x = samples_cauy[ , i]
    values <- unlist(lapply(loc_range, loglike_cauy, gam_val=gam_v, x_vec=x))
    ll_values_cauy = cbind(ll_values_cauy, values)
    colnames(ll_values_cauy)[i] = paste("logL-", as.character(i), sep="")
    
    # plot first ~15 likelihood functions (left)
    if (i < 2){
      plot(loc_range, values-min(values), type="l", xlab=expression(theta), 
           ylab="-logL (adj.)", cex=5, ylim=ylim,
           col=alpha("green", 0.3), 
           main=bquote("likelihood (n=" *.(length(x))* "," ~ gamma *"="*
                         .(gam_v)*")"),
           font.main=1)
      }
    else if (i <= plotted_iterations){
      lines(loc_range, values-min(values), cex=5, col=alpha("green", 0.3))}
    abline(v=loc_v, col="lightgreen", lwd=1, lty=2)
  }
  
  # plot density (right). z denotes the vector that contains the values of 
  # all 200 log-likelihoods evaluated at the true theta.
  z = ll_values_cauy[loc_position,]
  dens = density(z, bw="nrd0", from=0, to=500)
  plot(c(dens["x"], dens["y"]), type="l", col="lightgreen", xlab="-logL", 
       ylab="",main=bquote("density at " *theta* "=" *.(loc_v)))
}

# plot for both sample groups
plot_ll_cauy(samples_cauy_1, loc, gam_1, c(0,50))
plot_ll_cauy(samples_cauy_2, loc, gam_1, c(0,50))
plot_ll_cauy(samples_cauy_3, loc, gam_2, c(0,50))

```

As we can clearly see, in the low sample size case as well as the high variance case, the likelihood functions are less concentrated around the MLE (less "pointy"). It makes intuitive sense, that the resulting variance of the MLE must be lower. However, it would be difficult to try and spot the Fisher Information by just looking at the log-likelihood.


\newpage
### Level 2: The Score Function
Using the samples generated above, we can repeat the same process for the first derivative of the likelihood function. The resulting density estimations are asymptotic approximations of the analytical distribution of scores at the true parameter value. 

The Fisher Information is defined as the variance of the aforementioned distribution and can therefore be read directly from the graph. In the following graphic, we calculate the variance of scores explicitly to estimate the FI. Note, that this is an approximation of the first definition of the FI as mentioned in Equation (1).

```{r, echo=FALSE, fig.cap="score functions for the first 15 samples (left) and kernel density estimation over the score functions of all samples evaluated at the true parameter value (right). Arrow with length of 2*sqrt(FI)."}
par(mfrow=c(3,2), mar=c(4.1, 4.1, 2.1, 2.1))

# plotting functions for first derivative (1d) and 2d return FI estimate
plot_ll_1d_cauy <- function(samples_cauy, loc_v, gam_v, ylim){
  ll_1d_values_cauy = c()
  loc_position = which.min(abs(loc_range-loc_v))
  
  for (i in 1:ncol(samples_cauy)){  
    
    # calculate first derivative likelihood function values for each sample
    x = samples_cauy[ , i]
    values <- unlist(lapply(loc_range, loglike_cauy_1d, gam_val=gam_v,
                            x_vec=x))
    ll_1d_values_cauy = cbind(ll_1d_values_cauy, values)
    colnames(ll_1d_values_cauy)[i] = paste("dlogL-", as.character(i), sep="")
    
    # plot first derivative likelihood functions for first couple of samples
    if (i < 2){
      plot(loc_range, values, type="l", xlab=expression(theta), 
           ylab="-dlogL", cex=5, ylim= ylim,
           col=alpha("blue", 0.3),
           main=bquote("score value (n=" *.(length(x))* "," ~ gamma *"="*
                         .(gam_v)*")"), font.main=1)
      }
    else if (i <= plotted_iterations){
      lines(loc_range, values, cex=5, col=alpha("blue", 0.3))}
    abline(v=loc_v, col="lightblue", lwd=1, lty=2)
  }
  
  # plot density across all samples at true loc and calculate FI value
  z = ll_1d_values_cauy[loc_position,]
  
  fi = round(var(z),8) # FI is the variance of scores
  
  dens = density(z, bw="nrd0", from=-40, to=40)
  plot(c(dens["x"], dens["y"]), type="l", col="lightblue", xlab="-dlogL",
       ylab="", main=bquote("density at " *theta* "=" *.(loc_v) *" (FI="*
                         .(fi)*")"))
  
  Arrows(x0=loc_v-sqrt(fi), y0=(1/3)*max(unlist(dens["y"])), x1=loc_v+sqrt(fi),
        y1=(1/3)*max(unlist(dens["y"])), code=3, arr.length=0.1, 
        arr.type="simple", col="black")
  
  return(fi)
}

# plot for both sample groups
fi_estimates_1d <- c()
fi_estimates_1d = append(fi_estimates_1d, 
                         plot_ll_1d_cauy(samples_cauy_1, loc,gam_1, c(-15,15)))
fi_estimates_1d = append(fi_estimates_1d, 
                         plot_ll_1d_cauy(samples_cauy_2, loc,gam_1, c(-15,15)))
fi_estimates_1d = append(fi_estimates_1d, 
                         plot_ll_1d_cauy(samples_cauy_3, loc,gam_2, c(-15,15)))
```

The fact that the distribution of scores is centered around zero confirms that $\theta=0$ is a local minimum of the analytical likelihood.

A wider variance in scores, implies a greater Fisher Information and therefore a smaller Variance of the MLE. In the small $n$ case as well as the large $\gamma$ case, the scores are more closely concentrated around 0. This means that we are more likely to encounter a score value close to zero when evaluating the score function at the true parameter value. 

It is a tempting interpretation of a lower variance in scores to assume that it means, that we are more likely to pick the true parameter value as our MLE, and therefore the variance should in fact be lower. However, this is not true, since this observation holds for all possible parameter values in (close) proximity to the true $\theta$. Instead, if scores are often close to zero, regardless of the data that produced the likelihood function, it implies that the likelihood-function around the true parameter value is (on average) relatively insensitive to the parameter (i.e., more flat). A higher variance in scores on the other hand means that the function around the true $\theta$ is relatively sensitive to different inputs (i.e., sharper). 


\newpage
### Level 3: The Second derivative
However, when working with real data, we obviously do not have the necessary information to estimate the density function of score values. Instead, we use the second derivative of the log-likelihood. 

The average (mean) FI implied by the second derivative log-likelihoods evaluated at $\theta$ across all different samples is displayed in brackets. Note, that this relates to the second definition of the FI as mentioned in Equation (2) and (3).

```{r, echo=FALSE, fig.cap="second derivative log-likelihood functions for the first 15 samples (left) and kernel density estimation over the second derivative log-likelihood of all samples evaluated at the true parameter value (right). Arrow points at FI."}
par(mfrow=c(3,2), mar=c(4.1, 4.1, 2.1, 2.1))

fi_estimates_2d = c()
plot_ll_2d_cauy <- function(samples_cauy, loc_v, gam_v, ylim){
  ll_2d_values_cauy = c()
  loc_position = which.min(abs(loc_range-loc_v))
  
  for (i in 1:ncol(samples_cauy)){  
    
    # calculate second derivative likelihood function values for each sample
    x = samples_cauy[ , i]
    values <- unlist(lapply(loc_range, loglike_cauy_2d, gam_val=gam_v, 
                            x_vec=x))
    ll_2d_values_cauy = cbind(ll_2d_values_cauy, values)
    colnames(ll_2d_values_cauy)[i] = paste("ll-", as.character(i), sep="")
    
    # plot second derivative likelihood functions for first couple of samples
    if (i < 2){
      plot(loc_range, values, type="l", xlab=expression(theta), 
           ylab="-2dlogL", cex=5, ylim= ylim, col=alpha("red", 0.3),
           main=bquote("second derivative (n=" *.(length(x))* "," ~ gamma *"="*
                         .(gam_v)*")"),
           font.main=1)
      }
    else if (i <= plotted_iterations){
      lines(loc_range, values, cex=5, col=alpha("red", 0.3))}
    abline(v=loc_v, col="lightsalmon", lwd=1, lty=2)
  }
  
  # plot density across all samples at true loc
  z = ll_2d_values_cauy[loc_position,]
  
  fi = round(mean(z),8)
  fi_estimates_2d = c(fi_estimates_2d, fi)
  
  dens = density(z, bw="nrd0", from=-10, to=75)
  plot(c(dens["x"], dens["y"]), type="l", col="lightsalmon", xlab="-2dlogL",
       ylab="", main=bquote("density at " *theta* "=" *.(loc_v) *" (FI="*
                         .(fi)*")"))
  
  Arrows(x0=fi, y0=max(unlist(dens["y"]))/3, x1=fi,
        y1=0, code=2, arr.length=0.1, 
        arr.type="simple", col="black")
  
  return(fi)
}

fi_estimates = cbind(fi_estimates_1d, fi_estimates_2d)

# plot for both sample groups
fi_estimates_2d <- c()
fi_estimates_2d = append(fi_estimates_2d, 
                         plot_ll_2d_cauy(samples_cauy_1, loc,gam_1, c(-7,7)))
fi_estimates_2d = append(fi_estimates_2d, 
                         plot_ll_2d_cauy(samples_cauy_2, loc,gam_1, c(-7,7)))
fi_estimates_2d = append(fi_estimates_2d, 
                         plot_ll_2d_cauy(samples_cauy_3, loc,gam_2, c(-7,7)))
```

The second derivative measures the curvature of the likelihood function at the true parameter value and therefore more closely aligns with the idea behind our initial intuition about how the variance should behave according to the degree to which the likelihood is concentrated around the MLE. A high curvature implies that the score function is highly sensitive to changes in $\theta$ and therefore its root is relatively insensitive. 

As we can see here, each sample provides a different estimation for the Fisher Information but the results are asymptotically normally distributed around the true FI. 


\newpage
# Implications
In the simulations above, both methods provide similar results when it comes to estimating the Fisher Information from the generated samples:^[For reference: The analytical Fisher Information fro the Cauchy Distribution with $\gamma=1$ is given by $n/2$, which is close to our first two results [@pati16]]

```{r, echo=FALSE}
fi_estimates=cbind(c(paste("$n =$", n_1, ',  $\\gamma =$', gam_1), 
                     paste("$n =$", n_2, ',  $\\gamma =$', gam_1), 
                     paste("$n =$", n_1, ',  $\\gamma =$', gam_2)),
                   fi_estimates_1d, fi_estimates_2d,
                   round(fi_estimates_2d-fi_estimates_1d, 8),
                   round((fi_estimates_2d-fi_estimates_1d)/fi_estimates_1d, 8))
colnames(fi_estimates) = c("method", "var(1d)", "mean(2d)", "diff", "diff%")
knitr::kable( fi_estimates, booktabs = TRUE, caption = "average Fisher Information estimates for both methods", escape = FALSE)
```

So far we have worked under the assumption that the true parameter value $\theta$ is already known. This gave us a nice visual representation of the Fisher Information, and consequently the variance of the MLE. We now have two graphical interpretations of the Fisher Information:

1. The variance of the distribution of first derivatives at the _true_ $\theta$. 
2. The Fisher Information measures the sensitivity of the score-function (/the curvature of the likelihood-function) around the _true_ $\theta$.

However, when working with real data, we are observing the function at the _estimated_ parameter $\hat{\theta}$. Especially when working with a bumpy likelihood function, the MLE can be off. The Cauchy Distribution is one example of a pdf that can have multiple roots when solving for the MLE.^[Note that, while there are several ways to solve for the global maximum of the Cauchy Distribution [@10.1093/biomet/53.1-2.151], the following argument generalizes to other distributions that produce more complex multimodal likelihood functions.] A consequence of this is that the MLE significantly differs from the true parameter value.

This has an important implication: The MSE derived by calculating the Fisher information explicitly assumes that we will evaluate the function at or close to the true parameter. It does not take into account the possibility of a "false" local extremum. However, depending on the complexity of the likelihood function, and how well we are able to solve for it's global minimum, it is very likely, that our method will not return the "true" global minimum. The confidence intervals therefore may never cover the true parameter value, even if the true Fisher Information is used to estimate the MSE. 

```{r, echo=FALSE, fig.cap="Example of a likelihood function generated from a Cauchy distribution with a second local minimum at 223.07", fig.height = 3, fig.width=5}
set.seed(178221)
csample = rcauchy(10,0,1)
loc_grid = seq(-400,400,length.out=10001)
ll_value = unlist(lapply(loc_grid, loglike_cauy, gam_val=1, x_vec=csample))
 plot(loc_grid, ll_value, type="l", xlab=expression(theta), 
           ylab="-loglike", cex=5, col="green", 
           main=bquote("likelihood (n=10," ~ gamma *"=1)"))
```

An example of such a function with multiple minima is referenced in Figure (4). Note, that the Type-2 samples ($\gamma=50,\;  n=7$, second row) generated in our effort to visualize the FI stem from the same data generating process (dgp) that produced the log-likelihood in Figure (4). We will now examine the implications mentioned above, using the data from Figure (4) as an example.

```{r, echo=FALSE, fig.cap="The log-likelihood and its derivatives when the log-likelihood has multiple minima.", warning = FALSE}

plotted_iterations = 1
par(mfrow=c(3,2), mar=c(4.1, 4.1, 2.1, 2.1))

loc_length = 100001 # high sample rate necessary for accurate results! 
loc_range <- seq(loc-300, loc+300, length.out=loc_length)

# find minimum furthest from and closest to true parameter
csample_ll <- unlist(lapply(loc_range, loglike_cauy, gam_val=1, x_vec=csample))
minima = c()
for (i in 2:(length(csample_ll)-1)){
  if (csample_ll[i] <= csample_ll[i-1] && csample_ll[i] <= csample_ll[i+1]){
    minima = c(minima, loc_range[i])
  }
}
furthest_minimum = minima[which.max(abs(minima-loc))]+loc
closest_minimum = minima[which.min(abs(minima-loc))]+loc

# generate plots
samples_cauy_2b = cbind(csample, csample) # technicality

plot_ll_cauy(samples_cauy_2b, furthest_minimum, gam_1, c(0,100))
plot_ll_1d_cauy(samples_cauy_2b, furthest_minimum, gam_1, c(-4,2))
ofi=plot_ll_2d_cauy(samples_cauy_2b, furthest_minimum, gam_1, c(-0.5,1))
```

As Figure (5) shows, the second minimum is represented in the score function as a second root, that closely resembles the structure of the first one, making it difficult to discern the local minimum from a global one. Simple methods of gradient descent like the BFGS will in fact return the false local minimum when using a starting value >224.

Unsurprisingly, the same holds true for the second derivative. When evaluating the second derivative at the false minimum, the estimated FI is 1.99, implying a MSE of:
$$
MSE = \sqrt{\frac{1}{1.99}}=0.7085
$$
The true FI for the displayed likelihood is $n/2=5$, which yields a true variance of:
$$
MSE = \sqrt{\frac{1}{5}}=0.4472
$$
In either case, the resulting 99%-confidence interval would not cover the true parameter value, when we naively assume that we have found the global minimum.


\newpage
# References {-}




